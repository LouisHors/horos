import type { LLMProvider, LLMMessage, LLMConfig, LLMResponse, StreamCallback } from './providers/LLMProvider';
import { ProviderFactory } from './providers/ProviderFactory';

export type { LLMMessage, LLMConfig, LLMResponse, StreamCallback };

/**
 * LLMService - ç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
 * æ”¯æŒå¤šæä¾›å•†ï¼šOpenAI, Claude, DeepSeek, Moonshot, ç­‰
 * 
 * ä½¿ç”¨ç¤ºä¾‹ï¼š
 * ```typescript
 * // æ–¹å¼1: ä½¿ç”¨é»˜è®¤ Provider (ä»ç¯å¢ƒå˜é‡)
 * const service = new LLMService();
 * 
 * // æ–¹å¼2: æŒ‡å®š Provider
 * const service = new LLMService('claude', {
 *   apiKey: 'your-claude-key',
 *   defaultModel: 'claude-3-sonnet-20240229'
 * });
 * 
 * // æ–¹å¼3: ä¼ å…¥è‡ªå®šä¹‰ Provider
 * const service = new LLMService(customProvider);
 * ```
 */
export class LLMService {
  private provider: LLMProvider;

  constructor();
  constructor(provider: LLMProvider);
  constructor(type: string, config: { apiKey: string; baseURL?: string; defaultModel?: string });
  constructor(arg1?: LLMProvider | string, arg2?: { apiKey: string; baseURL?: string; defaultModel?: string }) {
    console.log('[LLMService] ğŸ”¨ åˆå§‹åŒ–...');
    if (arg1 && typeof arg1 !== 'string') {
      // ç›´æ¥ä¼ å…¥ Provider å®ä¾‹
      this.provider = arg1;
      console.log('[LLMService] âœ… ä½¿ç”¨ä¼ å…¥çš„ Provider');
    } else if (arg1 && arg2) {
      // ä¼ å…¥ç±»å‹å’Œé…ç½®
      console.log('[LLMService] ğŸ”§ ä»å‚æ•°åˆ›å»º Provider:', arg1);
      this.provider = ProviderFactory.createProvider(arg1 as any, arg2);
    } else {
      // ä»ç¯å¢ƒå˜é‡åˆ›å»º
      console.log('[LLMService] ğŸ”§ ä»ç¯å¢ƒå˜é‡åˆ›å»º Provider');
      this.provider = ProviderFactory.createFromEnv();
    }
    console.log('[LLMService] âœ… Provider:', this.provider.name);
  }

  /**
   * å‘é€èŠå¤©è¯·æ±‚
   */
  async chat(
    messages: LLMMessage[],
    config?: Partial<LLMConfig>
  ): Promise<LLMResponse> {
    const model = config?.model || 'GLM-4.7';
    console.log('[LLMService] ğŸ’¬ chat()', { model, msgCount: messages.length });
    try {
      const result = await this.provider.chat({
        model,
        messages,
        temperature: config?.temperature ?? 0.7,
        maxTokens: config?.maxTokens || 2000,
      });
      console.log('[LLMService] âœ… chat() æˆåŠŸ', { contentLength: result.content.length });
      return result;
    } catch (err) {
      console.error('[LLMService] âŒ chat() å¤±è´¥:', err);
      throw err;
    }
  }

  /**
   * æµå¼èŠå¤©
   */
  async chatStream(
    messages: LLMMessage[],
    onChunk: StreamCallback,
    config?: Partial<LLMConfig>
  ): Promise<LLMResponse> {
    return this.provider.chatStream(
      {
        model: config?.model || 'GLM-4.7',
        messages,
        temperature: config?.temperature ?? 0.7,
        maxTokens: config?.maxTokens || 2000,
      },
      onChunk
    );
  }

  /**
   * ç®€å•å®Œæˆï¼ˆå•è½®å¯¹è¯ï¼‰
   */
  async complete(prompt: string, config?: Partial<LLMConfig>): Promise<string> {
    console.log('[LLMService] ğŸ“ complete()', { promptLength: prompt.length });
    const response = await this.chat(
      [{ role: 'user', content: prompt }],
      config
    );
    return response.content;
  }

  /**
   * è·å–å½“å‰ Provider ä¿¡æ¯
   */
  getProvider(): LLMProvider {
    return this.provider;
  }

  /**
   * éªŒè¯é…ç½®æ˜¯å¦æœ‰æ•ˆ
   */
  async validate(): Promise<boolean> {
    return this.provider.validateConfig();
  }
}

// å»¶è¿Ÿåˆå§‹åŒ–çš„å•ä¾‹ - é¿å…æµè§ˆå™¨ç«¯æ¨¡å—åŠ è½½æ—¶å‡ºé”™
let _llmService: LLMService | null = null;

export const llmService = new Proxy({} as LLMService, {
  get(_, prop) {
    if (!_llmService) {
      console.log('[LLMService] ğŸ”„ å»¶è¿Ÿåˆå§‹åŒ–å•ä¾‹');
      _llmService = new LLMService();
    }
    return (_llmService as any)[prop];
  }
});

// å…¼å®¹ç›´æ¥è®¿é—®çš„ getter
export function getLLMService(): LLMService {
  if (!_llmService) {
    console.log('[LLMService] ğŸ”„ å»¶è¿Ÿåˆå§‹åŒ–å•ä¾‹ (getLLMService)');
    _llmService = new LLMService();
  }
  return _llmService;
}
