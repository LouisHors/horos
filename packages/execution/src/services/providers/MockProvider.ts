import { LLMProvider, LLMRequest, LLMResponse, ProviderConfig, StreamCallback } from './LLMProvider';

/**
 * MockProvider - ç”¨äºæµ‹è¯•çš„æ¨¡æ‹Ÿ LLM Provider
 * ä¸è°ƒç”¨çœŸå® APIï¼Œç›´æ¥è¿”å›æ¨¡æ‹Ÿå“åº”
 */
export class MockProvider implements LLMProvider {
  readonly name = 'Mock Provider';
  readonly capabilities = { 
    streaming: false, 
    functionCalling: false, 
    vision: false, 
    maxTokens: 4096 
  };
  
  config: ProviderConfig;
  
  constructor(config: ProviderConfig) {
    console.log('[MockProvider] ğŸ”¨ åˆ›å»ºå®ä¾‹');
    this.config = {
      ...config,
      defaultModel: config.defaultModel || 'mock-model',
    };
  }
  
  async chat(request: LLMRequest): Promise<LLMResponse> {
    console.log('[MockProvider] ğŸ’¬ chat()', { model: request.model, msgCount: request.messages.length });
    
    const lastMessage = request.messages[request.messages.length - 1];
    const content = `ã€æ¨¡æ‹Ÿå“åº”ã€‘æ”¶åˆ°æ¶ˆæ¯: "${lastMessage.content.slice(0, 50)}..."\n\nè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”ï¼Œç”¨äºéªŒè¯å·¥ä½œæµæ‰§è¡Œæµç¨‹ã€‚`;
    
    console.log('[MockProvider] âœ… è¿”å›æ¨¡æ‹Ÿå“åº”');
    return {
      content,
      model: request.model || this.config.defaultModel,
      usage: {
        promptTokens: 10,
        completionTokens: 20,
        totalTokens: 30,
      },
    };
  }
  
  async chatStream(request: LLMRequest, onChunk: StreamCallback): Promise<LLMResponse> {
    console.log('[MockProvider] ğŸ’¬ chatStream()');
    const content = 'ã€æ¨¡æ‹Ÿæµå¼å“åº”ã€‘è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”ã€‚';
    onChunk(content, false);
    onChunk('', true);
    
    return {
      content,
      model: request.model || this.config.defaultModel,
      usage: {
        promptTokens: 10,
        completionTokens: 20,
        totalTokens: 30,
      },
    };
  }
  
  async validateConfig(): Promise<boolean> {
    return true;
  }
}
